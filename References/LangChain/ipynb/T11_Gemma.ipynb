{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6470d3-defb-4fb8-8408-49772a2f7f7c",
   "metadata": {
    "id": "7b6470d3-defb-4fb8-8408-49772a2f7f7c",
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "\n",
    "_ = dotenv.load_dotenv(dotenv_path=\".env\", override=True)\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "local_path = \"./models\"\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    device_map=\"auto\", dtype=torch.bfloat16\n",
    ")\n",
    "model.save_pretrained(save_directory=local_path)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "tokenizer.save_pretrained(save_directory=local_path)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print(\"Time taken: %.2f\"%(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e9542-c883-4ee7-b712-0de18ef747d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "local_dir = \"./models\"\n",
    "llm = HuggingFacePipeline.from_model_id(task=\"text-generation\", model_id=local_dir,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.1})\n",
    "llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fbec6-11d6-4483-9c48-b024d0a45d24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Customer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "K51nyamjKkDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "K51nyamjKkDE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fcb1571c-1f26-418b-c461-acb9b4655683",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 31.50, Response: ```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": \"2\",\n",
      "  \"price_value\": \"leaf blower\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core import prompts\n",
    "import time\n",
    "\n",
    "review_template = \"\"\"\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else?\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = prompts.PromptTemplate.from_template(template=review_template)\n",
    "\n",
    "chain = prompt_template | llm.bind(skip_prompt=True)\n",
    "\n",
    "customer_review = \"\"\"\n",
    "This leaf blower is pretty amazing.  It has four settings:\n",
    "candle blower, gentle breeze, windy city, and tornado. It\n",
    "arrived in two days, just in time for my wife's anniversary\n",
    "present. I think my wife liked it so much she was speechless. So\n",
    "far I've been the only one using it, and I've been using it\n",
    "every other morning to clear the leaves on our lawn. It's\n",
    "slightly more expensive than the other leaf blowers out there,\n",
    "but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "review_response = chain.invoke(input={\"text\": customer_review})\n",
    "end_time = time.time() - start_time\n",
    "print(\"Duration: %.2f, Response: %s\"%(end_time, review_response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gpQJ3k4XNFHO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "collapsed": true,
    "id": "gpQJ3k4XNFHO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e44db91d-87a8-4c7e-abc9-7539e84e60b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 29.72, Response: ```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": \"2\",\n",
      "  \"price_value\": \"leaf blower\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph import graph\n",
    "import time\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    text: str\n",
    "    response: dict\n",
    "\n",
    "def review_node(state: GraphState):\n",
    "    response = chain.invoke(input={\"text\": state[\"text\"]})\n",
    "    return {\"response\": response}\n",
    "\n",
    "workflow = graph.StateGraph(state_schema=GraphState)\n",
    "workflow.add_node(node=\"review\", action=review_node)\n",
    "workflow.add_edge(start_key=graph.START, end_key=\"review\")\n",
    "workflow.add_edge(start_key=\"review\", end_key=graph.END)\n",
    "agents = workflow.compile()\n",
    "\n",
    "start_time = time.time()\n",
    "review_response = agents.invoke(input={\"text\": customer_review})\n",
    "end_time = time.time() - start_time\n",
    "print(\"Duration: %.2f, Response: %s\"%(end_time, review_response[\"response\"].content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widNkxb1NXM4",
   "metadata": {
    "id": "widNkxb1NXM4"
   },
   "source": [
    "## Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24685f03-6668-414d-b2e9-1a4b581610fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Adds `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"\n",
    "    Divide `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1dc2c7-2503-43b4-9d55-85d02901d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state\n",
    "import typing\n",
    "from langchain import messages\n",
    "import operator\n",
    "\n",
    "class MessagesState(typing.TypedDict):\n",
    "    messages: typing.Annotated[list[messages.AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7e2636-a16c-4088-97df-dc10ead2a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define llm node\n",
    "from langchain import messages\n",
    "\n",
    "tools = [wikipedia_tool]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "def llm_call(state: dict):\n",
    "    \"\"\"\n",
    "    LLM decides whether to call a tool or not.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(input=[\n",
    "                messages.SystemMessage(content=\"\"\"\n",
    "                    You're a helpful assistant tasked with performing\n",
    "                    Wikipedia search on inputs.\n",
    "                \"\"\")            \n",
    "            ] + state[\"messages\"])\n",
    "        ],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3afdc-7a11-4ae1-84d7-ed13d09141e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0099-c9aa-4a68-9f8e-d55c60fbbf38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae07737-8bce-465a-bd43-399c5802135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --no-cache-dir -qU \\\n",
    "    wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c42838-a8b4-45d9-82cb-4bb1640df012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define wikipedia tools\n",
    "from langchain import tools\n",
    "import wikipedia\n",
    "\n",
    "@tools.tool\n",
    "def wikipedia_tool(\n",
    "    query: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Use `query` to search Wikipedia for factual information.\n",
    "    Args:\n",
    "        query: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = wikipedia.search(query)[0]\n",
    "        if not title:\n",
    "            return \"No results found on Wikipedia.\"\n",
    "        summary = wikipedia.summary(title=title, sentences=5, auto_suggest=False,\n",
    "            redirect=True)\n",
    "    except Exception as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Wikipedia summary: {summary}\"\n",
    "\n",
    "summary = wikipedia_tool.invoke(input={\"query\": \"SWIFT\"})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540f027-0411-4d86-b0dd-77ec474c8906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from langgraph import graph, prebuilt\n",
    "import time\n",
    "\n",
    "class MessagesState(typing.TypedDict):\n",
    "    messages: typing.Annotated[list, graph.message.add_messages]\n",
    "    llm_calls: int\n",
    "\n",
    "workflow = graph.StateGraph(state_schema=MessagesState)\n",
    "\n",
    "tools = [wikipedia_tool]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "def llm_node(state: MessagesState):\n",
    "    messages = llm_with_tools.invoke(input=state[\"messages\"])\n",
    "    return {\"messages\": [messages]}\n",
    "workflow.add_node(node=\"llm\", action=llm_node)\n",
    "\n",
    "tools_node = prebuilt.ToolNode(tools=tools)\n",
    "workflow.add_node(node=\"tools\", action=tools_node)\n",
    "\n",
    "workflow.add_edge(start_key=graph.START, end_key=\"llm\")\n",
    "workflow.add_edge(start_key=\"llm\", end_key=\"tools\")\n",
    "workflow.add_edge(start_key=\"tools\", end_key=graph.END)\n",
    "agents = workflow.compile()\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762aef0-7d56-4c0e-b859-32d5b42950bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "response = agents.invoke(input={\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about SWIFT.\"}]\n",
    "})\n",
    "end_time = time.time() - start_time\n",
    "print(\"Duration: %.2f, Response: %s\"%(end_time, response))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
