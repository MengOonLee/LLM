{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475e035-0357-4748-9bc3-aaabd6579ebc",
   "metadata": {},
   "source": [
    "# LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859520ab-fbc8-4d80-9e97-621c99ad5a44",
   "metadata": {},
   "source": [
    "## Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e58c1-e964-4ebb-b4c3-cd741873d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs, environ\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from time import time\n",
    "\n",
    "_ = load_dotenv(dotenv_path=\".env\", override=True)\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "local_path = \"./models\"\n",
    "makedirs(local_path, exist_ok=True)\n",
    "\n",
    "start_time = time()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    token=environ[\"HF_TOKEN\"],\n",
    "    device_map=\"auto\", dtype=torch.bfloat16\n",
    ")\n",
    "model.save_pretrained(save_directory=local_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    token=environ[\"HF_TOKEN\"]\n",
    ")\n",
    "tokenizer.save_pretrained(save_directory=local_path)\n",
    "end_time = time() - start_time\n",
    "\n",
    "print(\"Time taken: %.2f\"%(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c8a46-7d96-4820-82c5-a919f82e5411",
   "metadata": {},
   "source": [
    "## Import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3e816-b8ff-4abf-a8eb-bb916db8c5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "local_dir = \"./models\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    task=\"text-generation\", model_id=local_dir,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.1})\n",
    "llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc6484-d360-46c3-9494-102f39bfb1cb",
   "metadata": {},
   "source": [
    "### State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4928f-404f-45a7-ba92-3697e4c11d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of agent.\n",
    "    Attr:\n",
    "        messages: A list of messages making up the conversation.\n",
    "        user_query: The initial query from the user.\n",
    "        tool_output: Any output from tools used by the agent.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e13b5-8939-43b5-a891-c742eedcb0ce",
   "metadata": {},
   "source": [
    "### Chatbot Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483b78db-9186-4e6d-bb1c-3897594ec9e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent says:\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful AI assistant. Always introduce yourself.\n",
      "\n",
      "What is SWIFT financial messages?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Hello there! I'm Gemma, a large language model created by the Gemma team at Google DeepMind. I'm here to help you with a wide range of tasks, and I'm particularly good at understanding and responding to questions about finance and related topics. \n",
      "\n",
      "**SWIFT financial messages** are a standardized system for transmitting financial messages between banks. Think of them as a digital postal service for the financial world. Here's a breakdown of what they are:\n",
      "\n",
      "* **What they do:** SWIFT (Society for Worldwide Interbank Financial Telecommunication) is a global network that allows banks to securely exchange information about payments, such as transfers of funds.\n",
      "* **How they work:** They use a standardized protocol (called SWIFT Standard Code) to format messages, ensuring they are understood by banks worldwide. These messages contain details like:\n",
      "    * **Sender and Receiver Details:** Bank account numbers, names, and other identifying information.\n",
      "    * **Transaction Details:** Amount, date, and type of transaction.\n",
      "    * **Confirmation Codes:** Used to verify the message's authenticity.\n",
      "* **Why they're important:** SWIFT is crucial for international trade, remittances, and various financial transactions. Itâ€™s a vital part of the global financial system.\n",
      "* **Key Features:**\n",
      "    * **Standardized:**  The protocol is widely adopted, making it easy for banks to communicate.\n",
      "    * **Secure:** SWIFT uses encryption and security measures to protect the data.\n",
      "    * **Reliable:** It's a well-established and reliable system.\n",
      "\n",
      "\n",
      "Do you have any specific questions about SWIFT that you'd like me to answer? For example, are you interested in:\n",
      "\n",
      "*   How SWIFT works in more detail?\n",
      "*   The different types of SWIFT messages?\n",
      "*   How SWIFT is used in a specific industry (like international trade)?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"You are a helpful AI assistant. Always introduce yourself.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Combine the prompt and LLM into a simple chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Let's ask agent a question!\n",
    "response = chain.invoke(input={\"input\": \"What is SWIFT financial messages?\"})\n",
    "\n",
    "print(\"AI Agent says:\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16e68a-c8d9-4a52-bcdf-5d17fec0fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Define a simple in-meory chat history store\n",
    "class ChatMessageHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def add_user_message(self, message: str) -> None:\n",
    "        self.messages.append(HumanMessage(content=message))\n",
    "\n",
    "    def add_ai_message(self, message: str) -> None:\n",
    "        self.messages.append(AIMessage(content=message))\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "    @property\n",
    "    def messages(self) -> list[BaseMessage]:\n",
    "        return self.messages\n",
    "\n",
    "    @messages.setter\n",
    "    def messages(self, value: list[BaseMessage]) -> None:\n",
    "        self._messages = value\n",
    "\n",
    "# Create a place to store conversation history\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> ConversationBufferMemory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ConversationBufferMemory(\n",
    "            chat_memory=ChatMessageHistory(),\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "    return store[session_id]\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"You're a helpful AI assistant. Always remember past conversations.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"), # This is where the memory is inserted\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Combine the new prompt and LLM into a chain\n",
    "chat_with_history = prompt_with_history | llm\n",
    "\n",
    "# Now integrate with RunnableWithMessageHistory\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    runnable=chat_with_history,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# We need to provide a session_id to store the history\n",
    "config = {\"configurable\": {\"session_id\": \"my_first_session\"}}\n",
    "\n",
    "response1 = conversational_chain.invoke(config=config,\n",
    "    input={\"input\": \"Hello, I'm Meng\"})\n",
    "print(f\"AI Agent says: {response1.content}\")\n",
    "\n",
    "response2 = conversational_chain.invoke(config=config,\n",
    "    input={\"input\": \"What's my name?\"})\n",
    "print(f\"AI Agent says: {response2.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
